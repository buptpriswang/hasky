{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import candidate_sampling_ops\n",
    "from tensorflow.python.ops import embedding_ops\n",
    "from tensorflow.python.ops import gen_nn_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import sparse_ops\n",
    "from tensorflow.python.ops import variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _sum_rows(x):\n",
    "  \"\"\"Returns a vector summing up each row of the matrix x.\"\"\"\n",
    "  # _sum_rows(x) is equivalent to math_ops.reduce_sum(x, 1) when x is\n",
    "  # a matrix.  The gradient of _sum_rows(x) is more efficient than\n",
    "  # reduce_sum(x, 1)'s gradient in today's implementation. Therefore,\n",
    "  # we use _sum_rows(x) in the nce_loss() computation since the loss\n",
    "  # is mostly used for training.\n",
    "  cols = array_ops.shape(x)[1]\n",
    "  ones_shape = array_ops.stack([cols, 1])\n",
    "  ones = array_ops.ones(ones_shape, x.dtype)\n",
    "  return array_ops.reshape(math_ops.matmul(x, ones), [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_sampled_ids_and_logits(weights,\n",
    "                            biases,\n",
    "                            labels,\n",
    "                            inputs,\n",
    "                            num_sampled,\n",
    "                            num_classes,\n",
    "                            num_true=1,\n",
    "                            sampled_values=None,\n",
    "                            subtract_log_q=True,\n",
    "                            remove_accidental_hits=False,\n",
    "                            partition_strategy=\"mod\",\n",
    "                            name=None):\n",
    "  \"\"\"Helper function for nce_loss and sampled_softmax_loss functions.\n",
    "\n",
    "  Computes sampled output training logits and labels suitable for implementing\n",
    "  e.g. noise-contrastive estimation (see nce_loss) or sampled softmax (see\n",
    "  sampled_softmax_loss).\n",
    "\n",
    "  Note: In the case where num_true > 1, we assign to each target class\n",
    "  the target probability 1 / num_true so that the target probabilities\n",
    "  sum to 1 per-example.\n",
    "\n",
    "  Args:\n",
    "    weights: A `Tensor` of shape `[num_classes, dim]`, or a list of `Tensor`\n",
    "        objects whose concatenation along dimension 0 has shape\n",
    "        `[num_classes, dim]`.  The (possibly-partitioned) class embeddings.\n",
    "    biases: A `Tensor` of shape `[num_classes]`.  The (possibly-partitioned)\n",
    "        class biases.\n",
    "    labels: A `Tensor` of type `int64` and shape `[batch_size,\n",
    "        num_true]`. The target classes.  Note that this format differs from\n",
    "        the `labels` argument of `nn.softmax_cross_entropy_with_logits`.\n",
    "    inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward\n",
    "        activations of the input network.\n",
    "    num_sampled: An `int`.  The number of classes to randomly sample per batch.\n",
    "    num_classes: An `int`. The number of possible classes.\n",
    "    num_true: An `int`.  The number of target classes per training example.\n",
    "    sampled_values: a tuple of (`sampled_candidates`, `true_expected_count`,\n",
    "        `sampled_expected_count`) returned by a `*_candidate_sampler` function.\n",
    "        (if None, we default to `log_uniform_candidate_sampler`)\n",
    "    subtract_log_q: A `bool`.  whether to subtract the log expected count of\n",
    "        the labels in the sample to get the logits of the true labels.\n",
    "        Default is True.  Turn off for Negative Sampling.\n",
    "    remove_accidental_hits:  A `bool`.  whether to remove \"accidental hits\"\n",
    "        where a sampled class equals one of the target classes.  Default is\n",
    "        False.\n",
    "    partition_strategy: A string specifying the partitioning strategy, relevant\n",
    "        if `len(weights) > 1`. Currently `\"div\"` and `\"mod\"` are supported.\n",
    "        Default is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.\n",
    "    name: A name for the operation (optional).\n",
    "  Returns:\n",
    "    out_logits, out_labels: `Tensor` objects each with shape\n",
    "        `[batch_size, num_true + num_sampled]`, for passing to either\n",
    "        `nn.sigmoid_cross_entropy_with_logits` (NCE) or\n",
    "        `nn.softmax_cross_entropy_with_logits` (sampled softmax).\n",
    "  \"\"\"\n",
    "\n",
    "  if isinstance(weights, variables.PartitionedVariable):\n",
    "    weights = list(weights)\n",
    "  if not isinstance(weights, list):\n",
    "    weights = [weights]\n",
    "\n",
    "  with ops.name_scope(name, \"compute_sampled_logits\",\n",
    "                      weights + [biases, inputs, labels]):\n",
    "    if labels.dtype != dtypes.int64:\n",
    "      labels = math_ops.cast(labels, dtypes.int64)\n",
    "    labels_flat = array_ops.reshape(labels, [-1])\n",
    "\n",
    "    # Sample the negative labels.\n",
    "    #   sampled shape: [num_sampled] tensor\n",
    "    #   true_expected_count shape = [batch_size, 1] tensor\n",
    "    #   sampled_expected_count shape = [num_sampled] tensor\n",
    "    if sampled_values is None:\n",
    "      sampled_values = candidate_sampling_ops.log_uniform_candidate_sampler(\n",
    "          true_classes=labels,\n",
    "          num_true=num_true,\n",
    "          num_sampled=num_sampled,\n",
    "          unique=True,\n",
    "          range_max=num_classes)\n",
    "    # NOTE: pylint cannot tell that 'sampled_values' is a sequence\n",
    "    # pylint: disable=unpacking-non-sequence\n",
    "    sampled, true_expected_count, sampled_expected_count = (\n",
    "        array_ops.stop_gradient(s) for s in sampled_values)\n",
    "    # pylint: enable=unpacking-non-sequence\n",
    "    sampled = math_ops.cast(sampled, dtypes.int64)\n",
    "\n",
    "    # labels_flat is a [batch_size * num_true] tensor\n",
    "    # sampled is a [num_sampled] int tensor\n",
    "    all_ids = array_ops.concat([labels_flat, sampled], 0)\n",
    "\n",
    "    # weights shape is [num_classes, dim]\n",
    "    all_w = embedding_ops.embedding_lookup(\n",
    "        weights, all_ids, partition_strategy=partition_strategy)\n",
    "    all_b = embedding_ops.embedding_lookup(\n",
    "        biases, all_ids, partition_strategy=partition_strategy)\n",
    "    # true_w shape is [batch_size * num_true, dim]\n",
    "    # true_b is a [batch_size * num_true] tensor\n",
    "    true_w = array_ops.slice(\n",
    "        all_w, [0, 0], array_ops.stack([array_ops.shape(labels_flat)[0], -1]))\n",
    "    true_b = array_ops.slice(all_b, [0], array_ops.shape(labels_flat))\n",
    "\n",
    "    # inputs shape is [batch_size, dim]\n",
    "    # true_w shape is [batch_size * num_true, dim]\n",
    "    # row_wise_dots is [batch_size, num_true, dim]\n",
    "    dim = array_ops.shape(true_w)[1:2]\n",
    "    new_true_w_shape = array_ops.concat([[-1, num_true], dim], 0)\n",
    "    row_wise_dots = math_ops.multiply(\n",
    "        array_ops.expand_dims(inputs, 1),\n",
    "        array_ops.reshape(true_w, new_true_w_shape))\n",
    "    # We want the row-wise dot plus biases which yields a\n",
    "    # [batch_size, num_true] tensor of true_logits.\n",
    "    dots_as_matrix = array_ops.reshape(row_wise_dots,\n",
    "                                       array_ops.concat([[-1], dim], 0))\n",
    "    true_logits = array_ops.reshape(_sum_rows(dots_as_matrix), [-1, num_true])\n",
    "    true_b = array_ops.reshape(true_b, [-1, num_true])\n",
    "    true_logits += true_b\n",
    "\n",
    "    # Lookup weights and biases for sampled labels.\n",
    "    #   sampled_w shape is [num_sampled, dim]\n",
    "    #   sampled_b is a [num_sampled] float tensor\n",
    "    sampled_w = array_ops.slice(\n",
    "        all_w, array_ops.stack([array_ops.shape(labels_flat)[0], 0]), [-1, -1])\n",
    "    sampled_b = array_ops.slice(all_b, array_ops.shape(labels_flat), [-1])\n",
    "\n",
    "    # inputs has shape [batch_size, dim]\n",
    "    # sampled_w has shape [num_sampled, dim]\n",
    "    # sampled_b has shape [num_sampled]\n",
    "    # Apply X*W'+B, which yields [batch_size, num_sampled]\n",
    "    sampled_logits = math_ops.matmul(\n",
    "        inputs, sampled_w, transpose_b=True) + sampled_b\n",
    "\n",
    "    if remove_accidental_hits:\n",
    "      acc_hits = candidate_sampling_ops.compute_accidental_hits(\n",
    "          labels, sampled, num_true=num_true)\n",
    "      acc_indices, acc_ids, acc_weights = acc_hits\n",
    "\n",
    "      # This is how SparseToDense expects the indices.\n",
    "      acc_indices_2d = array_ops.reshape(acc_indices, [-1, 1])\n",
    "      acc_ids_2d_int32 = array_ops.reshape(\n",
    "          math_ops.cast(acc_ids, dtypes.int32), [-1, 1])\n",
    "      sparse_indices = array_ops.concat([acc_indices_2d, acc_ids_2d_int32], 1,\n",
    "                                        \"sparse_indices\")\n",
    "      # Create sampled_logits_shape = [batch_size, num_sampled]\n",
    "      sampled_logits_shape = array_ops.concat(\n",
    "          [array_ops.shape(labels)[:1], array_ops.expand_dims(num_sampled, 0)],\n",
    "          0)\n",
    "      if sampled_logits.dtype != acc_weights.dtype:\n",
    "        acc_weights = math_ops.cast(acc_weights, sampled_logits.dtype)\n",
    "      sampled_logits += sparse_ops.sparse_to_dense(\n",
    "          sparse_indices,\n",
    "          sampled_logits_shape,\n",
    "          acc_weights,\n",
    "          default_value=0.0,\n",
    "          validate_indices=False)\n",
    "\n",
    "    if subtract_log_q:\n",
    "      # Subtract log of Q(l), prior probability that l appears in sampled.\n",
    "      true_logits -= math_ops.log(true_expected_count)\n",
    "      sampled_logits -= math_ops.log(sampled_expected_count)\n",
    "\n",
    "        \n",
    "    out_ids = array_ops.concat([labels, array_ops.tile(array_ops.expand_dims(sampled, 0), [array_ops.shape(labels)[0], 1])], 1)\n",
    "    # Construct output logits and labels. The true labels/logits start at col 0.\n",
    "    out_logits = array_ops.concat([true_logits, sampled_logits], 1)\n",
    "    \n",
    "\n",
    "  return out_ids, out_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "num_units = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_t = tf.constant(-1., shape=[vocab_size, num_units])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = tf.constant(-1., shape=[vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = tf.constant([1,2,3,4,5,6,7,8,9,10], tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = tf.expand_dims(labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_classes = vocab_size\n",
    "num_true = 1\n",
    "num_sampled = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampled_values = candidate_sampling_ops.log_uniform_candidate_sampler(\n",
    "          true_classes=labels,\n",
    "          num_true=1,\n",
    "          num_sampled=5,\n",
    "          unique=True,\n",
    "          range_max=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    sampled, true_expected_count, sampled_expected_count = (\n",
    "        array_ops.stop_gradient(s) for s in sampled_values)\n",
    "    # pylint: enable=unpacking-non-sequence\n",
    "    sampled = math_ops.cast(sampled, dtypes.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([124,  21,   3,  36, 455])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_expected_count.eval().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01378555,  0.29344296,  0.00667027,  0.08524187,  0.11156183], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_expected_count.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "      acc_hits = candidate_sampling_ops.compute_accidental_hits(\n",
    "          labels, sampled, num_true=num_true)\n",
    "      acc_indices, acc_ids, acc_weights = acc_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_indices.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_ids.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [10]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([89, 29, 76, 43, 30])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = tf.constant(2.0, shape=[10, num_units])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_ids, out_logits = compute_sampled_ids_and_logits(weights=w_t, biases=v, labels=labels, \n",
    "                        inputs=inputs, num_sampled=num_sampled, num_classes=num_classes, remove_accidental_hits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 21)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_logits.eval().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   4, 112,  67,  68,  11,   3,  14,   9, 284,   1, 950, 130,\n",
       "         12,   0, 717,   2,  10, 520,  51,  72],\n",
       "       [  2,   4, 112,  67,  68,  11,   3,  14,   9, 284,   1, 950, 130,\n",
       "         12,   0, 717,   2,  10, 520,  51,  72],\n",
       "       [  3,   4, 112,  67,  68,  11,   3,  14,   9, 284,   1, 950, 130,\n",
       "         12,   0, 717,   2,  10, 520,  51,  72],\n",
       "       [  4,   4, 112,  67,  68,  11,   3,  14,   9, 284,   1, 950, 130,\n",
       "         12,   0, 717,   2,  10, 520,  51,  72],\n",
       "       [  5,   4, 112,  67,  68,  11,   3,  14,   9, 284,   1, 950, 130,\n",
       "         12,   0, 717,   2,  10, 520,  51,  72],\n",
       "       [  6,   4, 112,  67,  68,  11,   3,  14,   9, 284,   1, 950, 130,\n",
       "         12,   0, 717,   2,  10, 520,  51,  72],\n",
       "       [  7,   4, 112,  67,  68,  11,   3,  14,   9, 284,   1, 950, 130,\n",
       "         12,   0, 717,   2,  10, 520,  51,  72],\n",
       "       [  8,   4, 112,  67,  68,  11,   3,  14,   9, 284,   1, 950, 130,\n",
       "         12,   0, 717,   2,  10, 520,  51,  72],\n",
       "       [  9,   4, 112,  67,  68,  11,   3,  14,   9, 284,   1, 950, 130,\n",
       "         12,   0, 717,   2,  10, 520,  51,  72],\n",
       "       [ 10,   4, 112,  67,  68,  11,   3,  14,   9, 284,   1, 950, 130,\n",
       "         12,   0, 717,   2,  10, 520,  51,  72]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_ids.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_flat = array_ops.reshape(labels, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_flat.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  6, 29, 27,  0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(sampled, 0).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_ids[:,0].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [10]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.slice(out_ids, [0, 0], [-1, 1]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.nn.log_uniform_candidate_sampler?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
