{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from IPython.display import Image\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "\n",
    "def image_show(image_path):\n",
    "  imshow(np.asarray(Image.open(image_path, 'r')))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".output_wrapper, .output {\n",
       "    height:auto !important;\n",
       "    max-height:10000px;  /* your desired max-height here */\n",
       "}\n",
       ".output_scroll {\n",
       "    box-shadow:none !important;\n",
       "    webkit-box-shadow:none !important;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper, .output {\n",
    "    height:auto !important;\n",
    "    max-height:10000px;  /* your desired max-height here */\n",
    "}\n",
    ".output_scroll {\n",
    "    box-shadow:none !important;\n",
    "    webkit-box-shadow:none !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow_version: 1.2.0-rc0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "image_model feature_name is None will get PreLogits\n",
      "image_feature: Tensor(\"InceptionResnetV2/Logits/Dropout/Identity:0\", shape=(?, 1536), dtype=float32)\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "image_model will get feature_name Conv2d_7b_1x1\n",
      "image_feature: Tensor(\"Flatten/Reshape:0\", shape=(?, 98304), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from /home/gezi/data/image_model_check_point/inception_resnet_v2_2016_08_30.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "restore image var from InceptionResnetV2 /home/gezi/data/image_model_check_point/inception_resnet_v2_2016_08_30.ckpt duration: 7.27134609222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/gezi/new/temp/image-caption/ai-challenger/model/showandtell/model.ckpt-73.4-301000\n",
      "INFO:tensorflow:Initializing vocabulary from file: /home/gezi/new/temp/image-caption/ai-challenger/tfrecord/seq-basic/vocab.txt\n",
      "INFO:tensorflow:Created vocabulary with 10148 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "restore meta grpah and model ok /home/gezi/new/temp/image-caption/ai-challenger/model/showandtell/model.ckpt-73.4-301000 duration: 12.3460309505\n",
      "ENCODE_UNK 1\n",
      "ENCODE_UNK 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/gezi/new/temp/image-caption/ai-challenger/model/bow/model.ckpt-49.3-202000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "restore meta grpah and model ok /home/gezi/new/temp/image-caption/ai-challenger/model/bow/model.ckpt-49.3-202000 duration: 2.06328105927\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from deepiu.util.text_predictor import TextPredictor\n",
    "from deepiu.util.sim_predictor import SimPredictor\n",
    "from deepiu.util import ids2text, text2ids\n",
    "import melt, gezi\n",
    "import numpy as np \n",
    "import traceback\n",
    "\n",
    "try:\n",
    "  import conf\n",
    "  from conf import TEXT_MAX_WORDS\n",
    "except Exception:\n",
    "  from deepiu.image_caption.conf import TEXT_MAX_WORDS\n",
    "\n",
    "image_dir = '/home/gezi/data2/data/ai_challenger/image_caption/pic/'\n",
    "image_file = '6275b5349168ac3fab6a493c509301d023cf39d3.jpg'\n",
    "\n",
    "image_model_checkpoint_path = '/home/gezi/data/image_model_check_point/inception_resnet_v2_2016_08_30.ckpt'\n",
    "model_dir = '/home/gezi/new/temp/image-caption/ai-challenger/model/showandtell/'\n",
    "sim_model_dir = '/home/gezi/new/temp/image-caption/ai-challenger/model/bow/'\n",
    "vocab_path = '/home/gezi/new/temp/image-caption/ai-challenger/tfrecord/seq-basic/vocab.txt'\n",
    "valid_dir = '/home/gezi/new/temp/image-caption/ai-challenger/tfrecord/seq-basic/valid'\n",
    "\n",
    "image_model_name='InceptionResnetV2'\n",
    "\n",
    "#if finetuned model, just  TextPredictor(model_dir, vocab_path)\n",
    "image_model = None\n",
    "if not melt.varname_in_checkpoint(image_model_name, model_dir):\n",
    "  image_model = melt.image.ImageModel(image_model_checkpoint_path, \n",
    "                                      model_name=image_model_name,\n",
    "                                      feature_name=None)\n",
    "  predictor = TextPredictor(model_dir, vocab_path, image_model=image_model)\n",
    "else:\n",
    "  predictor = TextPredictor(model_dir, vocab_path)\n",
    "  \n",
    "vocab = ids2text.vocab \n",
    "\n",
    "text2ids.init(vocab_path)\n",
    "\n",
    "sim_predictor = SimPredictor(sim_model_dir, \n",
    "                             image_model_checkpoint_path, \n",
    "                             image_model_name=image_model_name, \n",
    "                             image_model=image_model,\n",
    "                             index=-1)\n",
    "\n",
    "text_strs = np.load(os.path.join(valid_dir, 'distinct_text_strs.npy'))\n",
    "img2text = np.load(os.path.join(valid_dir, 'img2text.npy')).item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "  image_file = raw_input('image_file like 6275b5349168ac3fab6a493c509301d023cf39d3.jpg:')\n",
    "  if image_file == 'q':\n",
    "    break\n",
    "  if not image_file.endswith('.jpg'):\n",
    "    image_file += '.jpg'\n",
    "\n",
    "  image_path = os.path.join(image_dir, image_file)\n",
    "  print('image_path:', image_path)\n",
    "  image_show(image_path)\n",
    "\n",
    "  if not os.path.exists(image_path):\n",
    "    print('image path not find!')\n",
    "    continue\n",
    "\n",
    "  try:\n",
    "    hits = img2text[image_file]\n",
    "    texts = [text_strs[hit] for hit in hits]\n",
    "    for text in texts:\n",
    "      word_ids = text2ids.text2ids(text)\n",
    "      seg_text = text2ids.ids2text(word_ids, print_end=False)\n",
    "      print('label:', text, seg_text)\n",
    "      words_importance = sim_predictor.words_importance([word_ids])\n",
    "      words_importance = words_importance[0]\n",
    "      #print('word importance:')\n",
    "      for i in range(len(word_ids)):\n",
    "        if word_ids[i] == 0:\n",
    "          break \n",
    "        #print(vocab.key(int(word_ids[i])), words_importance[i], end='|')  \n",
    "      print()\n",
    "  except Exception:\n",
    "    print(traceback.format_exc(), file=sys.stderr)    \n",
    "    pass\n",
    "\n",
    "  image = melt.read_image(image_path)\n",
    "  word_ids, scores = predictor.word_ids([image])\n",
    "  word_id = word_ids[0]\n",
    "  score = scores[0]\n",
    "  print('best predict:', ids2text.translate(word_id[0]),  score[0], '/'.join([vocab.key(int(id)) for id in word_id[0] if id != vocab.end_id()]))\n",
    "  \n",
    "  l = [id for id in word_id[0] if id != vocab.end_id()]\n",
    "  l = gezi.pad(l, TEXT_MAX_WORDS)\n",
    "  words_importance = sim_predictor.words_importance([l])\n",
    "  words_importance = words_importance[0]\n",
    "\n",
    "  #print('word importance:')\n",
    "  for i in range(len(word_id[0])):\n",
    "    if word_id[0][i] == vocab.end_id():\n",
    "      break\n",
    "    #print(vocab.key(int(word_id[0][i])), words_importance[i], end='|')\n",
    "  print()\n",
    "\n",
    "  for i in range(len(word_id)):\n",
    "    if i > 0:\n",
    "      print('top%d predict:'%i, ids2text.translate(word_id[i]),  score[i], '/'.join([vocab.key(int(id)) for id in word_id[i] if id != vocab.end_id()]))\n",
    "      l = [id for id in word_id[i] if id != vocab.end_id()]\n",
    "      l = gezi.pad(l, TEXT_MAX_WORDS)\n",
    "      words_importance = sim_predictor.words_importance([l])\n",
    "      words_importance = words_importance[0]\n",
    "\n",
    "      #print('word importance:')\n",
    "      for j in range(len(word_id[i])):\n",
    "        if word_id[i][j] == vocab.end_id():\n",
    "          break\n",
    "        #print(vocab.key(int(word_id[i][j])), words_importance[j], end='|')\n",
    "      print()\n",
    "\n",
    "  scores, word_ids = sim_predictor.top_words([image], 50)\n",
    "  scores = scores[0]\n",
    "  word_ids = word_ids[0]\n",
    "  print('topwords of image:')\n",
    "  for word_id, score in zip(word_ids, scores):\n",
    "    print(vocab.key(int(word_id)), score, end='|')\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
